def _strip_comment_prefix(block: str, comment_prefixes: tuple[str, ...] = ("# ", "#")) -> str:
    """
    Remove leading comment prefixes from each line of a block.
    Args:
        block: Multiline string block to process.
        comment_prefixes: Tuple/list of prefix strings to remove from line starts.
    Returns:
        Block with comment prefixes removed from each line.
    """
    lines = block.splitlines()
    def _strip_line(line):
        for prefix in comment_prefixes:
            if line.lstrip().startswith(prefix):
                # Remove only one prefix per line, after optional leading whitespace
                i = line.find(prefix)
                return line[:i] + line[i+len(prefix):]
        return line
    return "\n".join(_strip_line(line) for line in lines)
from typing import Any, Dict, Tuple, Union, Optional

import yaml

from omnibase.model.model_enum_metadata import NodeMetadataField
from omnibase.protocol.protocol_canonical_serializer import ProtocolCanonicalSerializer


class CanonicalYAMLSerializer(ProtocolCanonicalSerializer):
    """
    Canonical YAML serializer implementing ProtocolCanonicalSerializer.
    Provides protocol-compliant, deterministic serialization and normalization for stamping, hashing, and idempotency.
    All field references use canonical Enums (NodeMetadataField).
    """

    def canonicalize_metadata_block(
        self,
        block: Union[Dict[str, Any], Any],
        volatile_fields: Tuple[NodeMetadataField, ...] = (
            NodeMetadataField.HASH,
            NodeMetadataField.LAST_MODIFIED_AT,
        ),
        placeholder: str = "<PLACEHOLDER>",
        sort_keys: bool = True,
        explicit_start: bool = True,
        explicit_end: bool = True,
        default_flow_style: bool = False,
        allow_unicode: bool = True,
        comment_prefix: str = "",
        **kwargs: Any,
    ) -> str:
        """
        Canonicalize a metadata block for deterministic YAML serialization and hash computation.
        - Accepts a dict or model instance.
        - Replaces volatile fields (e.g., hash, last_modified_at) with a protocol placeholder.
        - Returns the canonical YAML string (UTF-8, normalized line endings), with optional comment prefix.
        """
        if hasattr(block, "model_dump"):
            block_dict = block.model_dump(mode="json")
        else:
            block_dict = dict(block)
        # Protocol-compliant placeholders
        protocol_placeholders = {
            NodeMetadataField.HASH.value: "0" * 64,
            NodeMetadataField.LAST_MODIFIED_AT.value: "1970-01-01T00:00:00Z",
        }
        STRING_FIELDS: set[str] = {
            "description",
            "author",
            "created_at",
            "last_modified_at",
            "state_contract",
            "hash",
            "license",
            "runtime_language_hint",
            "namespace",
            "meta_type",
            "container_image_reference",
            "url",
            "commit_hash",
            "path",
            "signature",
            "algorithm",
            "signed_by",
            "issued_at",
            "data_residency_required",
            "data_classification",
            "level",
            "format",
            "binding",
            "protocol_required",
            "target",
            "name",
            "type",
        }
        LIST_FIELDS: set[str] = {
            "tags",
            "capabilities",
            "protocols_supported",
            "base_class",
            "dependencies",
            "inputs",
            "outputs",
            "environment",
            "os_requirements",
            "architectures",
            "compliance_profiles",
            "audit_events",
            "canonical_test_case_ids",
            "required_ci_tiers",
        }
        # Build a new normalized dict
        normalized_dict: Dict[str, Any] = {}
        for k, v in block_dict.items():
            # Replace volatile fields with protocol placeholder
            if k in protocol_placeholders:
                normalized_dict[k] = protocol_placeholders[k]
                continue
            # Convert NodeMetadataField to .value
            if isinstance(v, NodeMetadataField):
                v = v.value
            # Normalize string fields
            if k in STRING_FIELDS and (v is None or v == "null"):
                normalized_dict[k] = ""
                continue
            # Normalize list fields
            if k in LIST_FIELDS and (v is None or v == "null"):
                normalized_dict[k] = []
                continue
            normalized_dict[k] = v
        yaml_str = yaml.dump(
            normalized_dict,
            sort_keys=sort_keys,
            default_flow_style=default_flow_style,
            allow_unicode=allow_unicode,
            explicit_start=explicit_start,
            explicit_end=explicit_end,
        )
        yaml_str = yaml_str.replace('\xa0', ' ')
        yaml_str = yaml_str.replace("\r\n", "\n").replace("\r", "\n")
        assert "\r" not in yaml_str, "Carriage return found in canonical YAML string"
        yaml_str.encode("utf-8")  # Explicitly check UTF-8 encoding
        if comment_prefix:
            yaml_str = "\n".join(
                f"{comment_prefix}{line}" if line.strip() else ""
                for line in yaml_str.splitlines()
            )
        return yaml_str

    def normalize_body(self, body: str) -> str:
        """
        Canonical normalization for file body content.
        - Strips trailing spaces
        - Normalizes all line endings to '\n'
        - Ensures exactly one newline at EOF
        - Asserts only '\n' line endings are present
        """
        body = body.replace("\r\n", "\n").replace("\r", "\n")
        norm = body.rstrip(" \t\r\n") + "\n"
        assert "\r" not in norm, "Carriage return found after normalization"
        return norm

    def canonicalize_for_hash(
        self,
        block: Union[Dict[str, Any], Any],
        body: str,
        volatile_fields: Tuple[NodeMetadataField, ...] = (
            NodeMetadataField.HASH,
            NodeMetadataField.LAST_MODIFIED_AT,
        ),
        placeholder: str = "<PLACEHOLDER>",
        comment_prefix: str = "",
        **kwargs: Any,
    ) -> str:
        """
        Canonicalize the full content (block + body) for hash computation.
        - Returns the canonical string to be hashed.
        """
        meta_yaml = self.canonicalize_metadata_block(
            block,
            volatile_fields=volatile_fields,
            placeholder=placeholder,
            comment_prefix=comment_prefix,
            **kwargs,
        )
        norm_body = self.normalize_body(body)
        canonical = meta_yaml.rstrip("\n") + "\n\n" + norm_body.lstrip("\n")
        return canonical


normalize_body = CanonicalYAMLSerializer().normalize_body

def extract_metadata_block_and_body(
    content: str, open_delim: str, close_delim: str
) -> tuple[Optional[str], str]:
    """
    Canonical utility: Extract the metadata block (if present) and the rest of the file content.
    Returns (block_str or None, rest_of_content).
    """
    import re
    import logging
    logger = logging.getLogger("omnibase.canonical.canonical_serialization")

    # Accept both commented (# === ... ===) and non-commented (=== ... ===) delimiter forms for robust round-trip idempotency.
    # Allow every line between the delimiters to be optionally prefixed with a comment (e.g., '# '),
    # so it matches fully commented, non-commented, and mixed forms.
    # The block may be at the very start of the file, after a shebang, or after blank lines.
    pattern = (
        rf"(?ms)"  # multiline, dotall
        rf"^(?:[ \t\r\f\v]*\n)*"  # any number of leading blank lines/whitespace
        rf"[ \t\r\f\v]*(?:#\s*)?{re.escape(open_delim)}[ \t]*\n"  # open delimiter
        rf"((?:[ \t\r\f\v]*(?:#\s*)?.*\n)*?)"  # block content: any number of lines, each optionally commented
        rf"[ \t\r\f\v]*(?:#\s*)?{re.escape(close_delim)}[ \t]*\n?"  # close delimiter
    )

    match = re.search(pattern, content)
    if match:
        # Re-extract the full block, including delimiters
        block_start = match.start()
        block_end = match.end()
        block_str = content[block_start:block_end]
        rest = content[block_end:]
        # Strip comment prefixes from all lines (including delimiters) for downstream detection
        block_lines = block_str.splitlines()
        block_str_stripped = "\n".join(_strip_comment_prefix(line) for line in block_lines)
        logger.debug(
            f"extract_metadata_block_and_body: block_str=\n{block_str}\nrest=\n{rest}"
        )
        # Return the prefix-stripped block_str for downstream usage
        return block_str_stripped, rest
    else:
        logger.debug("extract_metadata_block_and_body: No block found")
        return None, content


def strip_block_delimiters_and_assert(
    lines: list[str], delimiters: set[str], context: str = ""
) -> str:
    """
    Canonical utility: Remove all lines that exactly match any delimiter. Assert none remain after filtering.
    Args:
        lines: List of lines (after comment prefix stripping)
        delimiters: Set of delimiter strings (imported from canonical constants)
        context: Optional context string for error messages
    Returns:
        Cleaned YAML string (no delimiters)
    Raises:
        AssertionError if any delimiter lines remain after filtering.
    """
    cleaned = [line for line in lines if line.strip() not in delimiters]
    remaining = [line for line in cleaned if line.strip() in delimiters]
    if remaining:
        raise AssertionError(
            f"Delimiter(s) still present after filtering in {context}: {remaining}"
        )
    return "\n".join(cleaned).strip()
